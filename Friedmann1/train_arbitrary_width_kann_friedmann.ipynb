{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'KAN_NN_fast' from 'c:\\\\Users\\\\JP\\\\Documents\\\\TU Berlin\\\\Master\\\\Code_clean\\\\AAAA\\\\KAN_NN_fast.py'>"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import Kan_NN\n",
    "import importlib\n",
    "importlib.reload(Kan_NN)\n",
    "import KAN_NN_fast\n",
    "importlib.reload(KAN_NN_fast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_test_loss(test_loader, model):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    running_loss = 0.\n",
    "    for batch, target in test_loader:\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(target, outputs)\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FRIEDMANN 1\n",
    "from sklearn.datasets import make_friedman1\n",
    "def get_loader(in_dim, noise, n_samples = 20000):\n",
    "    # Set the seed for reproducibility\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Generate the Friedmann dataset\n",
    "    X_train, y = make_friedman1(n_samples=int(n_samples * 0.8), n_features= in_dim, random_state=seed, noise=noise)\n",
    "    y_train = np.expand_dims(y, axis=1)\n",
    "    # Split into train and test sets (80% train, 20% test)\n",
    "    X_test, y = make_friedman1(n_samples=int(n_samples * 0.2), n_features= in_dim, random_state=seed, noise=0.0)\n",
    "    y_test = np.expand_dims(y, axis=1)\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create TensorDataset for train and test sets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Set batch size and create DataLoader for training and testing\n",
    "    batch_size = 1024\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10000, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 256\n",
      "256 1\n",
      "73985\n"
     ]
    }
   ],
   "source": [
    "model = KAN_NN_fast.Neural_Kan(shape = [5,256,1], h = [16])\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 256\n",
      "256 1\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul        35.16%      28.025ms        35.29%      28.126ms       2.164ms       6.78 Mb       6.78 Mb            13  \n",
      "                                              aten::bmm        14.14%      11.270ms        14.14%      11.273ms       1.879ms       3.13 Mb       3.13 Mb             6  \n",
      "                                        aten::unsqueeze        13.49%      10.754ms        13.51%      10.772ms     897.667us           0 b           0 b            12  \n",
      "                              Optimizer.step#RAdam.step         5.93%       4.730ms        10.18%       8.115ms       8.115ms     578.04 Kb    -578.02 Kb             1  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.09%      70.000us         9.74%       7.766ms       3.883ms       2.56 Mb    -544.12 Kb             2  \n",
      "                                           BmmBackward0         0.13%     107.000us         9.66%       7.696ms       3.848ms       3.09 Mb           0 b             2  \n",
      "                                           aten::einsum         0.29%     231.000us         6.07%       4.837ms       2.418ms      32.12 Kb           0 b             2  \n",
      "     autograd::engine::evaluate_function: ReluBackward0         0.09%      71.000us         4.68%       3.730ms       1.865ms      -3.00 Mb      -6.00 Mb             2  \n",
      "                                          ReluBackward0         0.04%      34.000us         4.59%       3.659ms       1.829ms       3.00 Mb           0 b             2  \n",
      "                               aten::threshold_backward         4.55%       3.625ms         4.55%       3.625ms       1.812ms       3.00 Mb       3.00 Mb             2  \n",
      "                    Optimizer.zero_grad#RAdam.zero_grad         4.09%       3.257ms         4.09%       3.257ms       3.257ms           0 b           0 b             1  \n",
      "                                          aten::reshape         0.56%     450.000us         3.49%       2.782ms     139.100us       3.00 Mb           0 b            20  \n",
      "      autograd::engine::evaluate_function: MulBackward0         0.15%     116.000us         2.92%       2.328ms       1.164ms      -2.91 Mb      -6.53 Mb             2  \n",
      "                                              aten::sum         2.71%       2.159ms         2.92%       2.327ms     290.875us     225.00 Kb     225.00 Kb             8  \n",
      "                                             aten::relu         0.10%      78.000us         2.65%       2.109ms       1.054ms       3.00 Mb           0 b             2  \n",
      "                                        aten::clamp_min         2.55%       2.031ms         2.55%       2.031ms       1.016ms       3.00 Mb       3.00 Mb             2  \n",
      "    autograd::engine::evaluate_function: SliceBackward0         0.08%      64.000us         2.54%       2.023ms     252.875us           0 b    -352.00 Kb             8  \n",
      "                                            aten::clone         0.36%     283.000us         2.51%       1.999ms     999.500us       3.00 Mb           0 b             2  \n",
      "                                         SliceBackward0         0.06%      45.000us         2.46%       1.959ms     244.875us     352.00 Kb           0 b             8  \n",
      "                                   aten::slice_backward         0.49%     392.000us         2.40%       1.914ms     239.250us     352.00 Kb           0 b             8  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 79.709ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_dim = 5\n",
    "model = KAN_NN_fast.Neural_Kan(shape = [in_dim,256,1], h = [16])  # replace with KAN_NN_fast.Neural_Kan(...) or any model\n",
    "model.train()\n",
    "\n",
    "inputs = torch.randn(32, in_dim)  # adjust input size as needed\n",
    "targets = torch.randn(32, 1)      # adjust target shape as needed\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU],  # or add CUDA if using GPU\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Print top 20 most expensive ops (including backward)\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1.0 [32]\n",
      "5 32\n",
      "32 1\n",
      "Neural_Kan(\n",
      "  (layers): Sequential(\n",
      "    (0): KAN_layer(\n",
      "      (layers): Sequential(\n",
      "        (0): Input_Linear()\n",
      "        (1): ReLU()\n",
      "        (2): Output_Linear()\n",
      "      )\n",
      "    )\n",
      "    (1): KAN_layer(\n",
      "      (layers): Sequential(\n",
      "        (0): Input_Linear()\n",
      "        (1): ReLU()\n",
      "        (2): Output_Linear()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Epoch [1/1000], Loss: 180.091332, test: 1.000000, lr: 0.001000\n",
      "Epoch [2/1000], Loss: 121.505061, test: 1.000000, lr: 0.001000\n",
      "Epoch [3/1000], Loss: 74.251704, test: 1.000000, lr: 0.001000\n",
      "Epoch [4/1000], Loss: 40.554458, test: 1.000000, lr: 0.001000\n",
      "Epoch [5/1000], Loss: 25.374580, test: 1.000000, lr: 0.001000\n",
      "Epoch [6/1000], Loss: 23.702708, test: 1.000000, lr: 0.001000\n",
      "Epoch [7/1000], Loss: 23.529076, test: 1.000000, lr: 0.001000\n",
      "Epoch [8/1000], Loss: 23.243073, test: 1.000000, lr: 0.001000\n",
      "Epoch [9/1000], Loss: 23.109497, test: 1.000000, lr: 0.001000\n",
      "Epoch [10/1000], Loss: 22.957366, test: 1.000000, lr: 0.001000\n",
      "Epoch [11/1000], Loss: 22.775448, test: 1.000000, lr: 0.001000\n",
      "Epoch [12/1000], Loss: 22.518411, test: 1.000000, lr: 0.001000\n",
      "Epoch [13/1000], Loss: 22.276703, test: 1.000000, lr: 0.001000\n",
      "Epoch [14/1000], Loss: 22.002210, test: 1.000000, lr: 0.001000\n",
      "Epoch [15/1000], Loss: 21.660559, test: 1.000000, lr: 0.001000\n",
      "Epoch [16/1000], Loss: 21.245150, test: 1.000000, lr: 0.001000\n",
      "Epoch [17/1000], Loss: 20.764343, test: 1.000000, lr: 0.001000\n",
      "Epoch [18/1000], Loss: 20.219843, test: 1.000000, lr: 0.001000\n",
      "Epoch [19/1000], Loss: 19.439460, test: 1.000000, lr: 0.001000\n",
      "Epoch [20/1000], Loss: 18.540219, test: 1.000000, lr: 0.001000\n",
      "Epoch [21/1000], Loss: 17.397472, test: 1.000000, lr: 0.001000\n",
      "Epoch [22/1000], Loss: 16.004928, test: 1.000000, lr: 0.001000\n",
      "Epoch [23/1000], Loss: 14.368268, test: 1.000000, lr: 0.001000\n",
      "Epoch [24/1000], Loss: 12.597315, test: 1.000000, lr: 0.001000\n",
      "Epoch [25/1000], Loss: 10.733484, test: 1.000000, lr: 0.001000\n",
      "Epoch [26/1000], Loss: 9.035015, test: 1.000000, lr: 0.001000\n",
      "Epoch [27/1000], Loss: 7.726473, test: 1.000000, lr: 0.001000\n",
      "Epoch [28/1000], Loss: 6.948722, test: 1.000000, lr: 0.001000\n",
      "Epoch [29/1000], Loss: 6.523709, test: 1.000000, lr: 0.001000\n",
      "Epoch [30/1000], Loss: 6.320798, test: 1.000000, lr: 0.001000\n",
      "Epoch [31/1000], Loss: 6.211314, test: 1.000000, lr: 0.001000\n",
      "Epoch [32/1000], Loss: 6.135236, test: 1.000000, lr: 0.001000\n",
      "Epoch [33/1000], Loss: 6.066509, test: 1.000000, lr: 0.001000\n",
      "Epoch [34/1000], Loss: 5.990638, test: 1.000000, lr: 0.001000\n",
      "Epoch [35/1000], Loss: 5.900763, test: 1.000000, lr: 0.001000\n",
      "Epoch [36/1000], Loss: 5.885363, test: 1.000000, lr: 0.001000\n",
      "Epoch [37/1000], Loss: 5.804621, test: 1.000000, lr: 0.001000\n",
      "Epoch [38/1000], Loss: 5.713670, test: 1.000000, lr: 0.001000\n",
      "Epoch [39/1000], Loss: 5.641021, test: 1.000000, lr: 0.001000\n",
      "Epoch [40/1000], Loss: 5.576188, test: 1.000000, lr: 0.001000\n",
      "Epoch [41/1000], Loss: 5.484582, test: 1.000000, lr: 0.001000\n",
      "Epoch [42/1000], Loss: 5.419714, test: 1.000000, lr: 0.001000\n",
      "Epoch [43/1000], Loss: 5.357470, test: 1.000000, lr: 0.001000\n",
      "Epoch [44/1000], Loss: 5.253167, test: 1.000000, lr: 0.001000\n",
      "Epoch [45/1000], Loss: 5.167256, test: 1.000000, lr: 0.001000\n",
      "Epoch [46/1000], Loss: 5.055574, test: 1.000000, lr: 0.001000\n",
      "Epoch [47/1000], Loss: 4.971130, test: 1.000000, lr: 0.001000\n",
      "Epoch [48/1000], Loss: 4.884671, test: 1.000000, lr: 0.001000\n",
      "Epoch [49/1000], Loss: 4.772040, test: 1.000000, lr: 0.001000\n",
      "Epoch [50/1000], Loss: 4.666972, test: 1.000000, lr: 0.001000\n",
      "Epoch [51/1000], Loss: 4.557321, test: 1.000000, lr: 0.001000\n",
      "Epoch [52/1000], Loss: 4.451799, test: 1.000000, lr: 0.001000\n",
      "Epoch [53/1000], Loss: 4.367839, test: 1.000000, lr: 0.001000\n",
      "Epoch [54/1000], Loss: 4.251314, test: 1.000000, lr: 0.001000\n",
      "Epoch [55/1000], Loss: 4.159289, test: 1.000000, lr: 0.001000\n",
      "Epoch [56/1000], Loss: 4.054972, test: 1.000000, lr: 0.001000\n",
      "Epoch [57/1000], Loss: 3.962926, test: 1.000000, lr: 0.001000\n",
      "Epoch [58/1000], Loss: 3.862299, test: 1.000000, lr: 0.001000\n",
      "Epoch [59/1000], Loss: 3.769451, test: 1.000000, lr: 0.001000\n",
      "Epoch [60/1000], Loss: 3.701968, test: 1.000000, lr: 0.001000\n",
      "Epoch [61/1000], Loss: 3.620681, test: 1.000000, lr: 0.001000\n",
      "Epoch [62/1000], Loss: 3.547590, test: 1.000000, lr: 0.001000\n",
      "Epoch [63/1000], Loss: 3.489560, test: 1.000000, lr: 0.001000\n",
      "Epoch [64/1000], Loss: 3.417952, test: 1.000000, lr: 0.001000\n",
      "Epoch [65/1000], Loss: 3.360055, test: 1.000000, lr: 0.001000\n",
      "Epoch [66/1000], Loss: 3.320834, test: 1.000000, lr: 0.001000\n",
      "Epoch [67/1000], Loss: 3.271901, test: 1.000000, lr: 0.001000\n",
      "Epoch [68/1000], Loss: 3.220943, test: 1.000000, lr: 0.001000\n",
      "Epoch [69/1000], Loss: 3.181819, test: 1.000000, lr: 0.001000\n",
      "Epoch [70/1000], Loss: 3.141601, test: 1.000000, lr: 0.001000\n",
      "Epoch [71/1000], Loss: 3.088170, test: 1.000000, lr: 0.001000\n",
      "Epoch [72/1000], Loss: 3.050077, test: 1.000000, lr: 0.001000\n",
      "Epoch [73/1000], Loss: 3.002387, test: 1.000000, lr: 0.001000\n",
      "Epoch [74/1000], Loss: 2.962462, test: 1.000000, lr: 0.001000\n",
      "Epoch [75/1000], Loss: 2.935704, test: 1.000000, lr: 0.001000\n",
      "Epoch [76/1000], Loss: 2.894886, test: 1.000000, lr: 0.001000\n",
      "Epoch [77/1000], Loss: 2.868472, test: 1.000000, lr: 0.001000\n",
      "Epoch [78/1000], Loss: 2.816343, test: 1.000000, lr: 0.001000\n",
      "Epoch [79/1000], Loss: 2.772148, test: 1.000000, lr: 0.001000\n",
      "Epoch [80/1000], Loss: 2.750790, test: 1.000000, lr: 0.001000\n",
      "Epoch [81/1000], Loss: 2.711755, test: 1.000000, lr: 0.001000\n",
      "Epoch [82/1000], Loss: 2.685889, test: 1.000000, lr: 0.001000\n",
      "Epoch [83/1000], Loss: 2.654261, test: 1.000000, lr: 0.001000\n",
      "Epoch [84/1000], Loss: 2.641612, test: 1.000000, lr: 0.001000\n",
      "Epoch [85/1000], Loss: 2.606264, test: 1.000000, lr: 0.001000\n",
      "Epoch [86/1000], Loss: 2.576140, test: 1.000000, lr: 0.001000\n",
      "Epoch [87/1000], Loss: 2.555588, test: 1.000000, lr: 0.001000\n",
      "Epoch [88/1000], Loss: 2.522559, test: 1.000000, lr: 0.001000\n",
      "Epoch [89/1000], Loss: 2.509583, test: 1.000000, lr: 0.001000\n",
      "Epoch [90/1000], Loss: 2.493925, test: 1.000000, lr: 0.001000\n",
      "Epoch [91/1000], Loss: 2.478288, test: 1.000000, lr: 0.001000\n",
      "Epoch [92/1000], Loss: 2.461826, test: 1.000000, lr: 0.001000\n",
      "Epoch [93/1000], Loss: 2.446248, test: 1.000000, lr: 0.001000\n",
      "Epoch [94/1000], Loss: 2.437512, test: 1.000000, lr: 0.001000\n",
      "Epoch [95/1000], Loss: 2.417913, test: 1.000000, lr: 0.001000\n",
      "Epoch [96/1000], Loss: 2.401370, test: 1.000000, lr: 0.001000\n",
      "Epoch [97/1000], Loss: 2.388279, test: 1.000000, lr: 0.001000\n",
      "Epoch [98/1000], Loss: 2.382350, test: 1.000000, lr: 0.001000\n",
      "Epoch [99/1000], Loss: 2.362921, test: 1.000000, lr: 0.001000\n",
      "Epoch [100/1000], Loss: 2.357419, test: 1.292214, lr: 0.001000\n",
      "Epoch [101/1000], Loss: 2.341753, test: 1.000000, lr: 0.001000\n",
      "Epoch [102/1000], Loss: 2.334921, test: 1.000000, lr: 0.001000\n",
      "Epoch [103/1000], Loss: 2.324886, test: 1.000000, lr: 0.001000\n",
      "Epoch [104/1000], Loss: 2.320094, test: 1.000000, lr: 0.001000\n",
      "Epoch [105/1000], Loss: 2.313192, test: 1.000000, lr: 0.001000\n",
      "Epoch [106/1000], Loss: 2.309845, test: 1.000000, lr: 0.001000\n",
      "Epoch [107/1000], Loss: 2.307875, test: 1.000000, lr: 0.001000\n",
      "Epoch [108/1000], Loss: 2.295780, test: 1.000000, lr: 0.001000\n",
      "Epoch [109/1000], Loss: 2.287333, test: 1.000000, lr: 0.001000\n",
      "Epoch [110/1000], Loss: 2.280924, test: 1.000000, lr: 0.001000\n",
      "Epoch [111/1000], Loss: 2.279120, test: 1.000000, lr: 0.001000\n",
      "Epoch [112/1000], Loss: 2.294552, test: 1.000000, lr: 0.001000\n",
      "Epoch [113/1000], Loss: 2.276389, test: 1.000000, lr: 0.001000\n",
      "Epoch [114/1000], Loss: 2.256969, test: 1.000000, lr: 0.001000\n",
      "Epoch [115/1000], Loss: 2.252475, test: 1.000000, lr: 0.001000\n",
      "Epoch [116/1000], Loss: 2.248430, test: 1.000000, lr: 0.001000\n",
      "Epoch [117/1000], Loss: 2.250860, test: 1.000000, lr: 0.001000\n",
      "Epoch [118/1000], Loss: 2.252087, test: 1.000000, lr: 0.001000\n",
      "Epoch [119/1000], Loss: 2.251230, test: 1.000000, lr: 0.001000\n",
      "Epoch [120/1000], Loss: 2.250721, test: 1.000000, lr: 0.001000\n",
      "Epoch [121/1000], Loss: 2.239175, test: 1.000000, lr: 0.001000\n",
      "Epoch [122/1000], Loss: 2.237269, test: 1.000000, lr: 0.001000\n",
      "Epoch [123/1000], Loss: 2.211039, test: 1.000000, lr: 0.001000\n",
      "Epoch [124/1000], Loss: 2.219473, test: 1.000000, lr: 0.001000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import dill\n",
    "n_samples = 20000\n",
    "widths = [[32]]\n",
    "in_dims = [5]\n",
    "noises = [1.]\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for width in widths:\n",
    "    for in_dim in in_dims:\n",
    "        for noise in noises:\n",
    "            print(in_dim, noise, width)\n",
    "            shape = [in_dim, width[0], 1]\n",
    "            train_loader, test_loader = get_loader(in_dim, noise,n_samples)\n",
    "            model = KAN_NN_fast.Neural_Kan(shape = shape, h = [16])\n",
    "            print(model)\n",
    "            epochs = 1000\n",
    "            model.train()\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay = 0.)\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            for epoch in range(epochs):\n",
    "                running_loss = 0.0 \n",
    "                for batch, target in train_loader:\n",
    "                    start_time = time.time()\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch.contiguous())\n",
    "                    loss = criterion(target, outputs)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "                avg_loss = running_loss / len(train_loader)\n",
    "                model.train_loss.append(avg_loss)\n",
    "                if not ((epoch + 1) % 100):\n",
    "                    test_l = compute_test_loss(test_loader, model)\n",
    "                    model.test_loss.append(test_l)\n",
    "                else:\n",
    "                    test_l = 1\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.6f}, test: {test_l:.6f}, lr: {optimizer.param_groups[0]['lr']:6f}\")\n",
    "            plt.plot(model.train_loss[-50:])\n",
    "            plt.title(f'train_loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            print(\"Training Complete!\")\n",
    "            #with open(f\"models/Friedmann_1_KAN_arbwidth{noise}_{in_dim}.dill\", \"wb\") as f:\n",
    "            #    dill.dump(model, f)\n",
    "            #with open(f\"models/KAN_{width[0]}_{noise}_{in_dim}.dill\", \"wb\") as f:\n",
    "            #    dill.dump(model, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
