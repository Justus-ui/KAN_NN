{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'KAN_NN_fast_repeat' from 'c:\\\\Users\\\\JP\\\\Documents\\\\TU Berlin\\\\Master\\\\Code_clean\\\\AAAA\\\\KAN_NN_fast_repeat.py'>"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import Kan_NN\n",
    "import importlib\n",
    "importlib.reload(Kan_NN)\n",
    "import KAN_NN_fast\n",
    "importlib.reload(KAN_NN_fast)\n",
    "import KAN_NN_fast_repeat\n",
    "importlib.reload(KAN_NN_fast_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_test_loss(test_loader, model):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    running_loss = 0.\n",
    "    for batch, target in test_loader:\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(target, outputs)\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FRIEDMANN 1\n",
    "from sklearn.datasets import make_friedman1\n",
    "def get_loader(in_dim, noise, n_samples = 20000):\n",
    "    # Set the seed for reproducibility\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Generate the Friedmann dataset\n",
    "    X_train, y = make_friedman1(n_samples=int(n_samples * 0.8), n_features= in_dim, random_state=seed, noise=noise)\n",
    "    y_train = np.expand_dims(y, axis=1)\n",
    "    # Split into train and test sets (80% train, 20% test)\n",
    "    X_test, y = make_friedman1(n_samples=int(n_samples * 0.2), n_features= in_dim, random_state=seed, noise=0.0)\n",
    "    y_test = np.expand_dims(y, axis=1)\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create TensorDataset for train and test sets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Set batch size and create DataLoader for training and testing\n",
    "    batch_size = 1024\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10000, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 256\n",
      "256 1\n",
      "73985\n"
     ]
    }
   ],
   "source": [
    "model = KAN_NN_fast.Neural_Kan(shape = [5,256,1], h = [16])\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 256\n",
      "256 1\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                              Optimizer.step#RAdam.step        18.58%      11.725ms        29.55%      18.645ms      18.645ms     578.04 Kb    -578.01 Kb             1  \n",
      "                                              aten::bmm        20.63%      13.018ms        20.64%      13.019ms       2.170ms       3.13 Mb       3.13 Mb             6  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.14%      91.000us        12.80%       8.077ms       4.038ms       2.56 Mb    -544.12 Kb             2  \n",
      "                                           aten::einsum         1.99%       1.257ms        12.77%       8.057ms       4.029ms      32.12 Kb           0 b             2  \n",
      "                                           BmmBackward0         0.10%      64.000us        12.66%       7.986ms       3.993ms       3.09 Mb           0 b             2  \n",
      "                                              aten::mul         7.41%       4.678ms         7.57%       4.776ms     367.385us       6.78 Mb       6.78 Mb            13  \n",
      "                                            aten::slice         7.07%       4.459ms         7.11%       4.483ms     249.056us           0 b           0 b            18  \n",
      "                                          aten::reshape         2.93%       1.849ms         6.53%       4.117ms     205.850us       3.00 Mb           0 b            20  \n",
      "    autograd::engine::evaluate_function: SliceBackward0         2.41%       1.521ms         5.82%       3.673ms     459.125us           0 b    -352.00 Kb             8  \n",
      "                                         aten::mse_loss         3.71%       2.338ms         5.29%       3.340ms       3.340ms         128 b           0 b             1  \n",
      "                                       aten::zeros_like         0.16%     104.000us         5.15%       3.249ms     191.118us     578.13 Kb           0 b            17  \n",
      "                                              aten::sum         3.74%       2.358ms         4.73%       2.983ms     372.875us     225.00 Kb     225.00 Kb             8  \n",
      "      autograd::engine::evaluate_function: MulBackward0         0.07%      42.000us         4.37%       2.754ms       1.377ms      -2.91 Mb      -7.03 Mb             2  \n",
      "                                       aten::empty_like         0.59%     374.000us         3.69%       2.328ms     116.400us       3.56 Mb           0 b            20  \n",
      "                                    aten::empty_strided         3.67%       2.318ms         3.67%       2.318ms      46.360us     578.14 Kb     578.14 Kb            50  \n",
      "     autograd::engine::evaluate_function: ReluBackward0         0.04%      26.000us         3.56%       2.248ms       1.124ms      -3.00 Mb      -6.00 Mb             2  \n",
      "                                          ReluBackward0         0.08%      50.000us         3.52%       2.222ms       1.111ms       3.00 Mb           0 b             2  \n",
      "                               aten::threshold_backward         3.44%       2.172ms         3.44%       2.172ms       1.086ms       3.00 Mb       3.00 Mb             2  \n",
      "                                         SliceBackward0         0.09%      55.000us         3.41%       2.152ms     269.000us     352.00 Kb           0 b             8  \n",
      "                                   aten::slice_backward         0.62%     389.000us         3.32%       2.097ms     262.125us     352.00 Kb           0 b             8  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 63.091ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_dim = 5\n",
    "model = KAN_NN_fast.Neural_Kan(shape = [in_dim,256,1], h = [16])  # replace with KAN_NN_fast.Neural_Kan(...) or any model\n",
    "model.train()\n",
    "\n",
    "inputs = torch.randn(32, in_dim)  # adjust input size as needed\n",
    "targets = torch.randn(32, 1)      # adjust target shape as needed\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU],  # or add CUDA if using GPU\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Print top 20 most expensive ops (including backward)\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1.0 [128]\n",
      "5 128\n",
      "128 1\n",
      "Neural_Kan(\n",
      "  (layers): Sequential(\n",
      "    (0): KAN_layer(\n",
      "      (layers): Sequential(\n",
      "        (0): Input_Linear()\n",
      "        (1): ReLU()\n",
      "        (2): Output_Linear()\n",
      "      )\n",
      "    )\n",
      "    (1): KAN_layer(\n",
      "      (layers): Sequential(\n",
      "        (0): Input_Linear()\n",
      "        (1): ReLU()\n",
      "        (2): Output_Linear()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Epoch [1/1000], Loss: 93.828364, test: 1.000000, lr: 0.001000\n",
      "Epoch [2/1000], Loss: 31.260930, test: 1.000000, lr: 0.001000\n",
      "Epoch [3/1000], Loss: 25.139443, test: 1.000000, lr: 0.001000\n",
      "Epoch [4/1000], Loss: 24.765115, test: 1.000000, lr: 0.001000\n",
      "Epoch [5/1000], Loss: 24.428970, test: 1.000000, lr: 0.001000\n",
      "Epoch [6/1000], Loss: 24.316508, test: 1.000000, lr: 0.001000\n",
      "Epoch [7/1000], Loss: 24.330039, test: 1.000000, lr: 0.001000\n",
      "Epoch [8/1000], Loss: 24.133897, test: 1.000000, lr: 0.001000\n",
      "Epoch [9/1000], Loss: 23.898387, test: 1.000000, lr: 0.001000\n",
      "Epoch [10/1000], Loss: 23.495801, test: 1.000000, lr: 0.001000\n",
      "Epoch [11/1000], Loss: 22.667371, test: 1.000000, lr: 0.001000\n",
      "Epoch [12/1000], Loss: 21.238191, test: 1.000000, lr: 0.001000\n",
      "Epoch [13/1000], Loss: 18.942331, test: 1.000000, lr: 0.001000\n",
      "Epoch [14/1000], Loss: 15.565277, test: 1.000000, lr: 0.001000\n",
      "Epoch [15/1000], Loss: 11.476580, test: 1.000000, lr: 0.001000\n",
      "Epoch [16/1000], Loss: 8.039684, test: 1.000000, lr: 0.001000\n",
      "Epoch [17/1000], Loss: 6.554069, test: 1.000000, lr: 0.001000\n",
      "Epoch [18/1000], Loss: 6.251729, test: 1.000000, lr: 0.001000\n",
      "Epoch [19/1000], Loss: 6.077184, test: 1.000000, lr: 0.001000\n",
      "Epoch [20/1000], Loss: 5.974408, test: 1.000000, lr: 0.001000\n",
      "Epoch [21/1000], Loss: 5.867684, test: 1.000000, lr: 0.001000\n",
      "Epoch [22/1000], Loss: 5.757302, test: 1.000000, lr: 0.001000\n",
      "Epoch [23/1000], Loss: 5.630297, test: 1.000000, lr: 0.001000\n",
      "Epoch [24/1000], Loss: 5.554004, test: 1.000000, lr: 0.001000\n",
      "Epoch [25/1000], Loss: 5.441127, test: 1.000000, lr: 0.001000\n",
      "Epoch [26/1000], Loss: 5.313444, test: 1.000000, lr: 0.001000\n",
      "Epoch [27/1000], Loss: 5.174600, test: 1.000000, lr: 0.001000\n",
      "Epoch [28/1000], Loss: 5.021691, test: 1.000000, lr: 0.001000\n",
      "Epoch [29/1000], Loss: 4.902387, test: 1.000000, lr: 0.001000\n",
      "Epoch [30/1000], Loss: 4.738628, test: 1.000000, lr: 0.001000\n",
      "Epoch [31/1000], Loss: 4.559479, test: 1.000000, lr: 0.001000\n",
      "Epoch [32/1000], Loss: 4.389075, test: 1.000000, lr: 0.001000\n",
      "Epoch [33/1000], Loss: 4.200575, test: 1.000000, lr: 0.001000\n",
      "Epoch [34/1000], Loss: 4.011725, test: 1.000000, lr: 0.001000\n",
      "Epoch [35/1000], Loss: 3.844663, test: 1.000000, lr: 0.001000\n",
      "Epoch [36/1000], Loss: 3.691109, test: 1.000000, lr: 0.001000\n",
      "Epoch [37/1000], Loss: 3.515958, test: 1.000000, lr: 0.001000\n",
      "Epoch [38/1000], Loss: 3.360295, test: 1.000000, lr: 0.001000\n",
      "Epoch [39/1000], Loss: 3.239747, test: 1.000000, lr: 0.001000\n",
      "Epoch [40/1000], Loss: 3.137329, test: 1.000000, lr: 0.001000\n",
      "Epoch [41/1000], Loss: 3.065354, test: 1.000000, lr: 0.001000\n",
      "Epoch [42/1000], Loss: 2.963525, test: 1.000000, lr: 0.001000\n",
      "Epoch [43/1000], Loss: 2.895137, test: 1.000000, lr: 0.001000\n",
      "Epoch [44/1000], Loss: 2.857503, test: 1.000000, lr: 0.001000\n",
      "Epoch [45/1000], Loss: 2.799486, test: 1.000000, lr: 0.001000\n",
      "Epoch [46/1000], Loss: 2.772555, test: 1.000000, lr: 0.001000\n",
      "Epoch [47/1000], Loss: 2.710587, test: 1.000000, lr: 0.001000\n",
      "Epoch [48/1000], Loss: 2.666228, test: 1.000000, lr: 0.001000\n",
      "Epoch [49/1000], Loss: 2.638808, test: 1.000000, lr: 0.001000\n",
      "Epoch [50/1000], Loss: 2.576004, test: 1.000000, lr: 0.001000\n",
      "Epoch [51/1000], Loss: 2.542239, test: 1.000000, lr: 0.001000\n",
      "Epoch [52/1000], Loss: 2.495785, test: 1.000000, lr: 0.001000\n",
      "Epoch [53/1000], Loss: 2.455580, test: 1.000000, lr: 0.001000\n",
      "Epoch [54/1000], Loss: 2.415446, test: 1.000000, lr: 0.001000\n",
      "Epoch [55/1000], Loss: 2.375891, test: 1.000000, lr: 0.001000\n",
      "Epoch [56/1000], Loss: 2.334386, test: 1.000000, lr: 0.001000\n",
      "Epoch [57/1000], Loss: 2.287030, test: 1.000000, lr: 0.001000\n",
      "Epoch [58/1000], Loss: 2.266332, test: 1.000000, lr: 0.001000\n",
      "Epoch [59/1000], Loss: 2.234749, test: 1.000000, lr: 0.001000\n",
      "Epoch [60/1000], Loss: 2.177955, test: 1.000000, lr: 0.001000\n",
      "Epoch [61/1000], Loss: 2.149445, test: 1.000000, lr: 0.001000\n",
      "Epoch [62/1000], Loss: 2.124974, test: 1.000000, lr: 0.001000\n",
      "Epoch [63/1000], Loss: 2.089125, test: 1.000000, lr: 0.001000\n",
      "Epoch [64/1000], Loss: 2.060626, test: 1.000000, lr: 0.001000\n",
      "Epoch [65/1000], Loss: 2.020476, test: 1.000000, lr: 0.001000\n",
      "Epoch [66/1000], Loss: 1.990677, test: 1.000000, lr: 0.001000\n",
      "Epoch [67/1000], Loss: 1.955671, test: 1.000000, lr: 0.001000\n",
      "Epoch [68/1000], Loss: 1.906890, test: 1.000000, lr: 0.001000\n",
      "Epoch [69/1000], Loss: 1.855510, test: 1.000000, lr: 0.001000\n",
      "Epoch [70/1000], Loss: 1.800979, test: 1.000000, lr: 0.001000\n",
      "Epoch [71/1000], Loss: 1.748422, test: 1.000000, lr: 0.001000\n",
      "Epoch [72/1000], Loss: 1.711725, test: 1.000000, lr: 0.001000\n",
      "Epoch [73/1000], Loss: 1.664524, test: 1.000000, lr: 0.001000\n",
      "Epoch [74/1000], Loss: 1.621489, test: 1.000000, lr: 0.001000\n",
      "Epoch [75/1000], Loss: 1.587798, test: 1.000000, lr: 0.001000\n",
      "Epoch [76/1000], Loss: 1.542895, test: 1.000000, lr: 0.001000\n",
      "Epoch [77/1000], Loss: 1.497281, test: 1.000000, lr: 0.001000\n",
      "Epoch [78/1000], Loss: 1.468417, test: 1.000000, lr: 0.001000\n",
      "Epoch [79/1000], Loss: 1.439558, test: 1.000000, lr: 0.001000\n",
      "Epoch [80/1000], Loss: 1.418324, test: 1.000000, lr: 0.001000\n",
      "Epoch [81/1000], Loss: 1.388634, test: 1.000000, lr: 0.001000\n",
      "Epoch [82/1000], Loss: 1.356175, test: 1.000000, lr: 0.001000\n",
      "Epoch [83/1000], Loss: 1.332403, test: 1.000000, lr: 0.001000\n",
      "Epoch [84/1000], Loss: 1.305593, test: 1.000000, lr: 0.001000\n",
      "Epoch [85/1000], Loss: 1.297111, test: 1.000000, lr: 0.001000\n",
      "Epoch [86/1000], Loss: 1.282598, test: 1.000000, lr: 0.001000\n",
      "Epoch [87/1000], Loss: 1.277210, test: 1.000000, lr: 0.001000\n",
      "Epoch [88/1000], Loss: 1.272198, test: 1.000000, lr: 0.001000\n",
      "Epoch [89/1000], Loss: 1.253794, test: 1.000000, lr: 0.001000\n",
      "Epoch [90/1000], Loss: 1.217914, test: 1.000000, lr: 0.001000\n",
      "Epoch [91/1000], Loss: 1.203989, test: 1.000000, lr: 0.001000\n",
      "Epoch [92/1000], Loss: 1.188993, test: 1.000000, lr: 0.001000\n",
      "Epoch [93/1000], Loss: 1.179300, test: 1.000000, lr: 0.001000\n",
      "Epoch [94/1000], Loss: 1.175276, test: 1.000000, lr: 0.001000\n",
      "Epoch [95/1000], Loss: 1.162047, test: 1.000000, lr: 0.001000\n",
      "Epoch [96/1000], Loss: 1.155682, test: 1.000000, lr: 0.001000\n",
      "Epoch [97/1000], Loss: 1.147159, test: 1.000000, lr: 0.001000\n",
      "Epoch [98/1000], Loss: 1.145949, test: 1.000000, lr: 0.001000\n",
      "Epoch [99/1000], Loss: 1.141785, test: 1.000000, lr: 0.001000\n",
      "Epoch [100/1000], Loss: 1.134685, test: 0.131796, lr: 0.001000\n",
      "Epoch [101/1000], Loss: 1.120171, test: 1.000000, lr: 0.001000\n",
      "Epoch [102/1000], Loss: 1.120410, test: 1.000000, lr: 0.001000\n",
      "Epoch [103/1000], Loss: 1.122180, test: 1.000000, lr: 0.001000\n",
      "Epoch [104/1000], Loss: 1.117335, test: 1.000000, lr: 0.001000\n",
      "Epoch [105/1000], Loss: 1.109189, test: 1.000000, lr: 0.001000\n",
      "Epoch [106/1000], Loss: 1.101005, test: 1.000000, lr: 0.001000\n",
      "Epoch [107/1000], Loss: 1.101125, test: 1.000000, lr: 0.001000\n",
      "Epoch [108/1000], Loss: 1.105566, test: 1.000000, lr: 0.001000\n",
      "Epoch [109/1000], Loss: 1.091074, test: 1.000000, lr: 0.001000\n",
      "Epoch [110/1000], Loss: 1.095585, test: 1.000000, lr: 0.001000\n",
      "Epoch [111/1000], Loss: 1.087001, test: 1.000000, lr: 0.001000\n",
      "Epoch [112/1000], Loss: 1.087092, test: 1.000000, lr: 0.001000\n",
      "Epoch [113/1000], Loss: 1.085841, test: 1.000000, lr: 0.001000\n",
      "Epoch [114/1000], Loss: 1.080519, test: 1.000000, lr: 0.001000\n",
      "Epoch [115/1000], Loss: 1.080948, test: 1.000000, lr: 0.001000\n",
      "Epoch [116/1000], Loss: 1.073809, test: 1.000000, lr: 0.001000\n",
      "Epoch [117/1000], Loss: 1.073097, test: 1.000000, lr: 0.001000\n",
      "Epoch [118/1000], Loss: 1.070359, test: 1.000000, lr: 0.001000\n",
      "Epoch [119/1000], Loss: 1.070136, test: 1.000000, lr: 0.001000\n",
      "Epoch [120/1000], Loss: 1.067926, test: 1.000000, lr: 0.001000\n",
      "Epoch [121/1000], Loss: 1.071397, test: 1.000000, lr: 0.001000\n",
      "Epoch [122/1000], Loss: 1.065735, test: 1.000000, lr: 0.001000\n",
      "Epoch [123/1000], Loss: 1.081493, test: 1.000000, lr: 0.001000\n",
      "Epoch [124/1000], Loss: 1.077575, test: 1.000000, lr: 0.001000\n",
      "Epoch [125/1000], Loss: 1.085809, test: 1.000000, lr: 0.001000\n",
      "Epoch [126/1000], Loss: 1.092748, test: 1.000000, lr: 0.001000\n",
      "Epoch [127/1000], Loss: 1.065969, test: 1.000000, lr: 0.001000\n",
      "Epoch [128/1000], Loss: 1.063860, test: 1.000000, lr: 0.001000\n",
      "Epoch [129/1000], Loss: 1.062379, test: 1.000000, lr: 0.001000\n",
      "Epoch [130/1000], Loss: 1.074285, test: 1.000000, lr: 0.001000\n",
      "Epoch [131/1000], Loss: 1.061360, test: 1.000000, lr: 0.001000\n",
      "Epoch [132/1000], Loss: 1.068138, test: 1.000000, lr: 0.001000\n",
      "Epoch [133/1000], Loss: 1.065186, test: 1.000000, lr: 0.001000\n",
      "Epoch [134/1000], Loss: 1.066999, test: 1.000000, lr: 0.001000\n",
      "Epoch [135/1000], Loss: 1.064984, test: 1.000000, lr: 0.001000\n",
      "Epoch [136/1000], Loss: 1.060865, test: 1.000000, lr: 0.001000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import dill\n",
    "n_samples = 20000\n",
    "widths = [[128]]\n",
    "in_dims = [5]\n",
    "noises = [1.]\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for width in widths:\n",
    "    for in_dim in in_dims:\n",
    "        for noise in noises:\n",
    "            print(in_dim, noise, width)\n",
    "            shape = [in_dim, width[0], 1]\n",
    "            train_loader, test_loader = get_loader(in_dim, noise,n_samples)\n",
    "            model = KAN_NN_fast.Neural_Kan(shape = shape, h = [32])\n",
    "            print(model)\n",
    "            epochs = 1000\n",
    "            model.train()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            for epoch in range(epochs):\n",
    "                running_loss = 0.0 \n",
    "                for batch, target in train_loader:\n",
    "                    start_time = time.time()\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch)\n",
    "                    loss = criterion(target, outputs)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "                avg_loss = running_loss / len(train_loader)\n",
    "                model.train_loss.append(avg_loss)\n",
    "                if not ((epoch + 1) % 100):\n",
    "                    test_l = compute_test_loss(test_loader, model)\n",
    "                    model.test_loss.append(test_l)\n",
    "                else:\n",
    "                    test_l = 1\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.6f}, test: {test_l:.6f}, lr: {optimizer.param_groups[0]['lr']:6f}\")\n",
    "            plt.plot(model.train_loss[-50:])\n",
    "            plt.title(f'train_loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            print(\"Training Complete!\")\n",
    "            with open(f\"models/Friedmann_1_KAN_arbwidth{noise}_{in_dim}.dill\", \"wb\") as f:\n",
    "                dill.dump(model, f)\n",
    "            #with open(f\"models/KAN_{width[0]}_{noise}_{in_dim}.dill\", \"wb\") as f:\n",
    "            #    dill.dump(model, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
