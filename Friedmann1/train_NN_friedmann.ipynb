{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLayerNN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, depth, N_params):\n",
    "        super(MLayerNN, self).__init__()\n",
    "        self.test_loss = []\n",
    "        self.train_loss = []\n",
    "        # Initialize layer sizes list\n",
    "        layer_dims = [in_dim]  # First layer is input dim\n",
    "        h = int(N_params / ((in_dim + out_dim) * (depth)))\n",
    "        while (in_dim * h + (depth-3) * h**2 + h * out_dim) > N_params:\n",
    "            h -= 1\n",
    "        #h = 4096 ### Number params hidden layer\n",
    "        layer_dims += [int(h)] * (depth - 2)  # Set all hidden layers to hidden_dim\n",
    "        layer_dims.append(out_dim)  # Last layer is output dim\n",
    "        print(layer_dims)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layer_dims[i-1], layer_dims[i]) for i in range(1,depth)\n",
    "        ])\n",
    "        self.params = sum(p.numel() for p in self.parameters() if p.ndimension() > 1)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))  # Apply ReLU activation\n",
    "        return self.layers[-1](x)  # Final layer without activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FRIEDMANN 1\n",
    "from sklearn.datasets import make_friedman1\n",
    "def get_loader(in_dim, noise, n_samples = 20000):\n",
    "    # Set the seed for reproducibility\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Generate the Friedmann dataset\n",
    "    X_train, y = make_friedman1(n_samples=int(n_samples * 0.8), n_features= in_dim, random_state=seed, noise=noise)\n",
    "    y_train = np.expand_dims(y, axis=1)\n",
    "    # Split into train and test sets (80% train, 20% test)\n",
    "    X_test, y = make_friedman1(n_samples=int(n_samples * 0.2), n_features= in_dim, random_state=seed, noise=0.0)\n",
    "    y_test = np.expand_dims(y, axis=1)\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create TensorDataset for train and test sets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Set batch size and create DataLoader for training and testing\n",
    "    batch_size = 1024\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_test_loss(test_loader, model):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    running_loss = 0.\n",
    "    for batch, target in test_loader:\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(target, outputs)\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 12500, 1]\n",
      "87501\n"
     ]
    }
   ],
   "source": [
    "model = MLayerNN(5, 1, depth = 3 , N_params = 3*75000)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 12500, 1]\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                              Optimizer.step#RAdam.step        18.39%       1.225ms        44.35%       2.954ms       2.954ms     683.62 Kb    -683.60 Kb             1  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         1.13%      75.000us        19.92%       1.327ms     663.500us     341.68 Kb      -1.53 Mb             2  \n",
      "     autograd::engine::evaluate_function: ReluBackward0         0.26%      17.000us        19.59%       1.305ms       1.305ms      -1.53 Mb      -3.05 Mb             1  \n",
      "                                          ReluBackward0         0.17%      11.000us        19.34%       1.288ms       1.288ms       1.53 Mb           0 b             1  \n",
      "                               aten::threshold_backward        19.17%       1.277ms        19.17%       1.277ms       1.277ms       1.53 Mb       1.53 Mb             1  \n",
      "                                         AddmmBackward0         0.63%      42.000us        12.36%     823.000us     411.500us       1.81 Mb           0 b             2  \n",
      "                                               aten::mm        10.58%     705.000us        10.58%     705.000us     235.000us       1.81 Mb       1.81 Mb             3  \n",
      "                                           aten::linear         0.27%      18.000us         8.51%     567.000us     283.500us       1.53 Mb           0 b             2  \n",
      "                                            aten::addmm         4.28%     285.000us         7.09%     472.000us     236.000us       1.53 Mb       1.53 Mb             2  \n",
      "                                            aten::lerp_         6.44%     429.000us         6.44%     429.000us     107.250us           0 b           0 b             4  \n",
      "                                              aten::sum         6.11%     407.000us         6.40%     426.000us     142.000us      48.83 Kb      48.83 Kb             3  \n",
      "                                               aten::to         1.10%      73.000us         5.21%     347.000us      16.524us          68 b           4 b            21  \n",
      "                                             aten::add_         2.52%     168.000us         4.53%     302.000us      37.750us           0 b         -16 b             8  \n",
      "                                         aten::_to_copy         2.88%     192.000us         4.40%     293.000us      17.235us          68 b           0 b            17  \n",
      "                                       aten::zeros_like         1.25%      83.000us         4.02%     268.000us      29.778us     683.73 Kb           0 b             9  \n",
      "                                            aten::copy_         3.56%     237.000us         3.56%     237.000us      12.474us           0 b           0 b            19  \n",
      "                                             aten::relu         0.26%      17.000us         3.33%     222.000us     222.000us       1.53 Mb           0 b             1  \n",
      "                                        aten::clamp_min         3.08%     205.000us         3.08%     205.000us     205.000us       1.53 Mb       1.53 Mb             1  \n",
      "                                             aten::mul_         1.86%     124.000us         2.99%     199.000us      49.750us           0 b         -16 b             4  \n",
      "                                              aten::div         1.50%     100.000us         2.67%     178.000us      44.500us     341.80 Kb     341.78 Kb             4  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 6.661ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_dim = 5\n",
    "model = model = MLayerNN(5, 1, depth = 3 , N_params = 3*75000)  # replace with KAN_NN_fast.Neural_Kan(...) or any model\n",
    "model.train()\n",
    "\n",
    "inputs = torch.randn(32, in_dim)  # adjust input size as needed\n",
    "targets = torch.randn(32, 1)      # adjust target shape as needed\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU],  # or add CUDA if using GPU\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Print top 20 most expensive ops (including backward)\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 12500, 1]\n",
      "Epoch [1/1000], Loss: 60.983673, test: 29.283024, lr: 0.001000\n",
      "Epoch [2/1000], Loss: 16.265793, test: 12.011430, lr: 0.001000\n",
      "Epoch [3/1000], Loss: 9.712362, test: 7.008840, lr: 0.001000\n",
      "Epoch [4/1000], Loss: 7.390205, test: 5.596724, lr: 0.001000\n",
      "Epoch [5/1000], Loss: 6.452611, test: 5.007449, lr: 0.001000\n",
      "Epoch [6/1000], Loss: 5.876878, test: 4.392658, lr: 0.001000\n",
      "Epoch [7/1000], Loss: 5.277183, test: 3.783074, lr: 0.001000\n",
      "Epoch [8/1000], Loss: 4.663867, test: 3.162368, lr: 0.001000\n",
      "Epoch [9/1000], Loss: 4.032223, test: 2.541576, lr: 0.001000\n",
      "Epoch [10/1000], Loss: 3.383700, test: 1.955919, lr: 0.001000\n",
      "Epoch [11/1000], Loss: 2.806610, test: 1.456875, lr: 0.001000\n",
      "Epoch [12/1000], Loss: 2.349221, test: 1.112297, lr: 0.001000\n",
      "Epoch [13/1000], Loss: 2.046224, test: 0.893815, lr: 0.001000\n",
      "Epoch [14/1000], Loss: 1.851869, test: 0.739606, lr: 0.001000\n",
      "Epoch [15/1000], Loss: 1.706030, test: 0.623170, lr: 0.001000\n",
      "Epoch [16/1000], Loss: 1.576598, test: 0.512677, lr: 0.001000\n",
      "Epoch [17/1000], Loss: 1.487508, test: 0.427351, lr: 0.001000\n",
      "Epoch [18/1000], Loss: 1.410239, test: 0.372945, lr: 0.001000\n",
      "Epoch [19/1000], Loss: 1.343350, test: 0.327330, lr: 0.001000\n",
      "Epoch [20/1000], Loss: 1.299925, test: 0.279772, lr: 0.001000\n",
      "Epoch [21/1000], Loss: 1.268171, test: 0.273748, lr: 0.001000\n",
      "Epoch [22/1000], Loss: 1.231261, test: 0.220957, lr: 0.001000\n",
      "Epoch [23/1000], Loss: 1.205155, test: 0.219028, lr: 0.001000\n",
      "Epoch [24/1000], Loss: 1.184766, test: 0.186919, lr: 0.001000\n",
      "Epoch [25/1000], Loss: 1.156229, test: 0.181192, lr: 0.001000\n",
      "Epoch [26/1000], Loss: 1.134939, test: 0.146592, lr: 0.001000\n",
      "Epoch [27/1000], Loss: 1.120659, test: 0.140430, lr: 0.001000\n",
      "Epoch [28/1000], Loss: 1.107692, test: 0.133973, lr: 0.001000\n",
      "Epoch [29/1000], Loss: 1.095179, test: 0.124165, lr: 0.001000\n",
      "Epoch [30/1000], Loss: 1.083983, test: 0.110444, lr: 0.001000\n",
      "Epoch [31/1000], Loss: 1.077774, test: 0.104149, lr: 0.001000\n",
      "Epoch [32/1000], Loss: 1.073459, test: 0.099040, lr: 0.001000\n",
      "Epoch [33/1000], Loss: 1.065787, test: 0.085812, lr: 0.001000\n",
      "Epoch [34/1000], Loss: 1.054517, test: 0.089953, lr: 0.001000\n",
      "Epoch [35/1000], Loss: 1.046456, test: 0.077736, lr: 0.001000\n",
      "Epoch [36/1000], Loss: 1.042559, test: 0.075684, lr: 0.001000\n",
      "Epoch [37/1000], Loss: 1.038896, test: 0.074147, lr: 0.001000\n",
      "Epoch [38/1000], Loss: 1.032831, test: 0.066722, lr: 0.001000\n",
      "Epoch [39/1000], Loss: 1.033261, test: 0.067568, lr: 0.001000\n",
      "Epoch [40/1000], Loss: 1.025647, test: 0.062591, lr: 0.001000\n",
      "Epoch [41/1000], Loss: 1.024797, test: 0.058674, lr: 0.001000\n",
      "Epoch [42/1000], Loss: 1.022185, test: 0.056199, lr: 0.001000\n",
      "Epoch [43/1000], Loss: 1.018281, test: 0.055079, lr: 0.001000\n",
      "Epoch [44/1000], Loss: 1.014342, test: 0.052927, lr: 0.001000\n",
      "Epoch [45/1000], Loss: 1.010687, test: 0.049866, lr: 0.001000\n",
      "Epoch [46/1000], Loss: 1.009588, test: 0.049811, lr: 0.001000\n",
      "Epoch [47/1000], Loss: 1.007471, test: 0.047358, lr: 0.001000\n",
      "Epoch [48/1000], Loss: 1.007178, test: 0.045048, lr: 0.001000\n",
      "Epoch [49/1000], Loss: 1.004418, test: 0.052443, lr: 0.001000\n",
      "Epoch [50/1000], Loss: 1.004166, test: 0.044801, lr: 0.001000\n",
      "Epoch [51/1000], Loss: 1.002455, test: 0.044061, lr: 0.001000\n",
      "Epoch [52/1000], Loss: 0.997732, test: 0.041867, lr: 0.001000\n",
      "Epoch [53/1000], Loss: 0.998544, test: 0.052439, lr: 0.001000\n",
      "Epoch [54/1000], Loss: 0.996198, test: 0.040572, lr: 0.001000\n",
      "Epoch [55/1000], Loss: 0.998414, test: 0.042684, lr: 0.001000\n",
      "Epoch [56/1000], Loss: 0.998399, test: 0.041573, lr: 0.001000\n",
      "Epoch [57/1000], Loss: 0.999066, test: 0.049006, lr: 0.001000\n",
      "Epoch [58/1000], Loss: 0.994913, test: 0.037658, lr: 0.001000\n",
      "Epoch [59/1000], Loss: 0.992016, test: 0.037957, lr: 0.001000\n",
      "Epoch [60/1000], Loss: 0.995152, test: 0.040217, lr: 0.001000\n",
      "Epoch [61/1000], Loss: 0.987683, test: 0.038937, lr: 0.001000\n",
      "Epoch [62/1000], Loss: 0.991852, test: 0.036759, lr: 0.001000\n",
      "Epoch [63/1000], Loss: 0.986518, test: 0.035683, lr: 0.001000\n",
      "Epoch [64/1000], Loss: 0.994238, test: 0.035331, lr: 0.001000\n",
      "Epoch [65/1000], Loss: 0.986829, test: 0.041569, lr: 0.001000\n",
      "Epoch [66/1000], Loss: 0.990265, test: 0.035562, lr: 0.001000\n",
      "Epoch [67/1000], Loss: 0.986587, test: 0.044277, lr: 0.001000\n",
      "Epoch [68/1000], Loss: 0.986998, test: 0.032372, lr: 0.001000\n",
      "Epoch [69/1000], Loss: 0.991771, test: 0.049817, lr: 0.001000\n",
      "Epoch [70/1000], Loss: 0.988894, test: 0.039861, lr: 0.001000\n",
      "Epoch [71/1000], Loss: 0.984235, test: 0.031127, lr: 0.001000\n",
      "Epoch [72/1000], Loss: 0.981632, test: 0.030432, lr: 0.001000\n",
      "Epoch [73/1000], Loss: 0.976185, test: 0.033023, lr: 0.001000\n",
      "Epoch [74/1000], Loss: 0.978250, test: 0.030785, lr: 0.001000\n",
      "Epoch [75/1000], Loss: 0.976900, test: 0.031976, lr: 0.001000\n",
      "Epoch [76/1000], Loss: 0.982014, test: 0.032919, lr: 0.001000\n",
      "Epoch [77/1000], Loss: 0.975831, test: 0.029486, lr: 0.001000\n",
      "Epoch [78/1000], Loss: 0.974859, test: 0.028576, lr: 0.001000\n",
      "Epoch [79/1000], Loss: 0.975790, test: 0.032752, lr: 0.001000\n",
      "Epoch [80/1000], Loss: 0.973945, test: 0.029467, lr: 0.001000\n",
      "Epoch [81/1000], Loss: 0.973790, test: 0.040356, lr: 0.001000\n",
      "Epoch [82/1000], Loss: 0.978338, test: 0.041934, lr: 0.001000\n",
      "Epoch [83/1000], Loss: 0.976271, test: 0.036296, lr: 0.001000\n",
      "Epoch [84/1000], Loss: 0.972607, test: 0.035407, lr: 0.001000\n",
      "Epoch [85/1000], Loss: 0.971458, test: 0.031491, lr: 0.001000\n",
      "Epoch [86/1000], Loss: 0.976888, test: 0.029494, lr: 0.001000\n",
      "Epoch [87/1000], Loss: 0.973621, test: 0.029756, lr: 0.001000\n",
      "Epoch [88/1000], Loss: 0.967501, test: 0.027441, lr: 0.001000\n",
      "Epoch [89/1000], Loss: 0.966650, test: 0.034138, lr: 0.001000\n",
      "Epoch [90/1000], Loss: 0.969533, test: 0.030385, lr: 0.001000\n",
      "Epoch [91/1000], Loss: 0.970388, test: 0.028463, lr: 0.001000\n",
      "Epoch [92/1000], Loss: 0.969628, test: 0.041745, lr: 0.001000\n",
      "Epoch [93/1000], Loss: 0.974123, test: 0.045390, lr: 0.001000\n",
      "Epoch [94/1000], Loss: 0.972670, test: 0.028246, lr: 0.001000\n",
      "Epoch [95/1000], Loss: 0.966362, test: 0.035697, lr: 0.001000\n",
      "Epoch [96/1000], Loss: 0.966454, test: 0.026055, lr: 0.001000\n",
      "Epoch [97/1000], Loss: 0.970354, test: 0.038410, lr: 0.001000\n",
      "Epoch [98/1000], Loss: 0.966132, test: 0.027004, lr: 0.001000\n",
      "Epoch [99/1000], Loss: 0.967052, test: 0.025357, lr: 0.001000\n",
      "Epoch [100/1000], Loss: 0.970938, test: 0.037769, lr: 0.001000\n",
      "Epoch [101/1000], Loss: 0.969029, test: 0.027644, lr: 0.001000\n",
      "Epoch [102/1000], Loss: 0.966062, test: 0.029118, lr: 0.001000\n",
      "Epoch [103/1000], Loss: 0.964097, test: 0.027360, lr: 0.001000\n",
      "Epoch [104/1000], Loss: 0.966148, test: 0.041571, lr: 0.001000\n",
      "Epoch [105/1000], Loss: 0.962562, test: 0.045637, lr: 0.001000\n",
      "Epoch [106/1000], Loss: 0.963171, test: 0.030111, lr: 0.001000\n",
      "Epoch [107/1000], Loss: 0.963481, test: 0.034720, lr: 0.001000\n",
      "Epoch [108/1000], Loss: 0.959665, test: 0.027342, lr: 0.001000\n",
      "Epoch [109/1000], Loss: 0.967244, test: 0.035172, lr: 0.001000\n",
      "Epoch [110/1000], Loss: 0.965482, test: 0.036048, lr: 0.001000\n",
      "Epoch [111/1000], Loss: 0.966719, test: 0.030195, lr: 0.001000\n",
      "Epoch [112/1000], Loss: 0.960491, test: 0.028288, lr: 0.001000\n",
      "Epoch [113/1000], Loss: 0.958768, test: 0.039043, lr: 0.001000\n",
      "Epoch [114/1000], Loss: 0.960584, test: 0.034497, lr: 0.001000\n",
      "Epoch [115/1000], Loss: 0.959024, test: 0.030499, lr: 0.001000\n",
      "Epoch [116/1000], Loss: 0.960343, test: 0.037066, lr: 0.001000\n",
      "Epoch [117/1000], Loss: 0.960353, test: 0.029099, lr: 0.001000\n",
      "Epoch [118/1000], Loss: 0.960766, test: 0.029001, lr: 0.001000\n",
      "Epoch [119/1000], Loss: 0.958004, test: 0.028552, lr: 0.001000\n",
      "Epoch [120/1000], Loss: 0.958868, test: 0.031278, lr: 0.001000\n",
      "Epoch [121/1000], Loss: 0.962756, test: 0.029211, lr: 0.001000\n",
      "Epoch [122/1000], Loss: 0.958889, test: 0.036413, lr: 0.001000\n",
      "Epoch [123/1000], Loss: 0.957047, test: 0.024745, lr: 0.001000\n",
      "Epoch [124/1000], Loss: 0.958079, test: 0.036095, lr: 0.001000\n",
      "Epoch [125/1000], Loss: 0.961607, test: 0.029210, lr: 0.001000\n",
      "Epoch [126/1000], Loss: 0.955513, test: 0.031320, lr: 0.001000\n",
      "Epoch [127/1000], Loss: 0.952288, test: 0.028774, lr: 0.001000\n",
      "Epoch [128/1000], Loss: 0.955541, test: 0.031038, lr: 0.001000\n",
      "Epoch [129/1000], Loss: 0.960035, test: 0.028636, lr: 0.001000\n",
      "Epoch [130/1000], Loss: 0.960692, test: 0.032298, lr: 0.001000\n",
      "Epoch [131/1000], Loss: 0.953134, test: 0.028452, lr: 0.001000\n",
      "Epoch [132/1000], Loss: 0.955191, test: 0.062694, lr: 0.001000\n",
      "Epoch [133/1000], Loss: 0.965207, test: 0.034830, lr: 0.001000\n",
      "Epoch [134/1000], Loss: 0.951518, test: 0.040468, lr: 0.001000\n",
      "Epoch [135/1000], Loss: 0.952425, test: 0.027004, lr: 0.001000\n",
      "Epoch [136/1000], Loss: 0.949136, test: 0.025995, lr: 0.001000\n",
      "Epoch [137/1000], Loss: 0.952265, test: 0.030772, lr: 0.001000\n",
      "Epoch [138/1000], Loss: 0.951356, test: 0.027051, lr: 0.001000\n",
      "Epoch [139/1000], Loss: 0.959815, test: 0.038200, lr: 0.001000\n",
      "Epoch [140/1000], Loss: 0.959420, test: 0.027575, lr: 0.001000\n",
      "Epoch [141/1000], Loss: 0.951741, test: 0.028689, lr: 0.001000\n",
      "Epoch [142/1000], Loss: 0.954240, test: 0.029340, lr: 0.001000\n",
      "Epoch [143/1000], Loss: 0.958296, test: 0.031174, lr: 0.001000\n",
      "Epoch [144/1000], Loss: 0.952742, test: 0.037037, lr: 0.001000\n",
      "Epoch [145/1000], Loss: 0.950454, test: 0.027894, lr: 0.001000\n",
      "Epoch [146/1000], Loss: 0.951029, test: 0.035922, lr: 0.001000\n",
      "Epoch [147/1000], Loss: 0.953411, test: 0.030680, lr: 0.001000\n",
      "Epoch [148/1000], Loss: 0.952841, test: 0.053330, lr: 0.001000\n",
      "Epoch [149/1000], Loss: 0.960493, test: 0.035806, lr: 0.001000\n",
      "Epoch [150/1000], Loss: 0.949313, test: 0.049833, lr: 0.001000\n",
      "Epoch [151/1000], Loss: 0.954864, test: 0.034442, lr: 0.001000\n",
      "Epoch [152/1000], Loss: 0.954311, test: 0.038603, lr: 0.001000\n",
      "Epoch [153/1000], Loss: 0.959777, test: 0.073750, lr: 0.001000\n",
      "Epoch [154/1000], Loss: 0.971721, test: 0.038176, lr: 0.001000\n",
      "Epoch [155/1000], Loss: 0.959121, test: 0.030808, lr: 0.001000\n",
      "Epoch [156/1000], Loss: 0.942907, test: 0.046276, lr: 0.001000\n",
      "Epoch [157/1000], Loss: 0.949443, test: 0.064633, lr: 0.001000\n",
      "Epoch [158/1000], Loss: 0.955577, test: 0.031447, lr: 0.001000\n",
      "Epoch [159/1000], Loss: 0.949485, test: 0.036190, lr: 0.001000\n",
      "Epoch [160/1000], Loss: 0.953906, test: 0.037864, lr: 0.001000\n",
      "Epoch [161/1000], Loss: 0.947171, test: 0.047557, lr: 0.001000\n",
      "Epoch [162/1000], Loss: 0.959024, test: 0.075517, lr: 0.001000\n",
      "Epoch [163/1000], Loss: 0.948581, test: 0.028705, lr: 0.001000\n",
      "Epoch [164/1000], Loss: 0.952143, test: 0.064163, lr: 0.001000\n",
      "Epoch [165/1000], Loss: 0.975472, test: 0.055998, lr: 0.001000\n",
      "Epoch [166/1000], Loss: 0.945267, test: 0.030493, lr: 0.001000\n",
      "Epoch [167/1000], Loss: 0.958146, test: 0.044244, lr: 0.001000\n",
      "Epoch [168/1000], Loss: 0.950835, test: 0.032088, lr: 0.001000\n",
      "Epoch [169/1000], Loss: 0.944178, test: 0.031028, lr: 0.001000\n",
      "Epoch [170/1000], Loss: 0.949498, test: 0.059676, lr: 0.001000\n",
      "Epoch [171/1000], Loss: 0.950537, test: 0.027450, lr: 0.001000\n",
      "Epoch [172/1000], Loss: 0.944810, test: 0.028287, lr: 0.001000\n",
      "Epoch [173/1000], Loss: 0.946403, test: 0.069885, lr: 0.001000\n",
      "Epoch [174/1000], Loss: 0.978172, test: 0.072520, lr: 0.001000\n",
      "Epoch [175/1000], Loss: 0.964951, test: 0.042795, lr: 0.001000\n",
      "Epoch [176/1000], Loss: 0.953027, test: 0.028934, lr: 0.001000\n",
      "Epoch [177/1000], Loss: 0.954419, test: 0.033289, lr: 0.001000\n",
      "Epoch [178/1000], Loss: 0.943115, test: 0.027157, lr: 0.001000\n",
      "Epoch [179/1000], Loss: 0.942970, test: 0.043104, lr: 0.001000\n",
      "Epoch [180/1000], Loss: 0.942421, test: 0.041135, lr: 0.001000\n",
      "Epoch [181/1000], Loss: 0.952313, test: 0.032560, lr: 0.001000\n",
      "Epoch [182/1000], Loss: 0.942893, test: 0.038229, lr: 0.001000\n",
      "Epoch [183/1000], Loss: 0.940083, test: 0.033100, lr: 0.001000\n",
      "Epoch [184/1000], Loss: 0.945794, test: 0.079502, lr: 0.001000\n",
      "Epoch [185/1000], Loss: 0.948076, test: 0.030095, lr: 0.001000\n",
      "Epoch [186/1000], Loss: 0.940350, test: 0.032049, lr: 0.001000\n",
      "Epoch [187/1000], Loss: 0.941435, test: 0.036180, lr: 0.001000\n",
      "Epoch [188/1000], Loss: 0.952506, test: 0.028753, lr: 0.001000\n",
      "Epoch [189/1000], Loss: 0.951252, test: 0.036967, lr: 0.001000\n",
      "Epoch [190/1000], Loss: 0.943129, test: 0.042561, lr: 0.001000\n",
      "Epoch [191/1000], Loss: 0.944529, test: 0.035946, lr: 0.001000\n",
      "Epoch [192/1000], Loss: 0.942557, test: 0.038627, lr: 0.001000\n",
      "Epoch [193/1000], Loss: 0.941185, test: 0.034061, lr: 0.001000\n",
      "Epoch [194/1000], Loss: 0.957705, test: 0.046829, lr: 0.001000\n",
      "Epoch [195/1000], Loss: 0.952383, test: 0.049605, lr: 0.001000\n",
      "Epoch [196/1000], Loss: 0.942567, test: 0.027903, lr: 0.001000\n",
      "Epoch [197/1000], Loss: 0.941335, test: 0.027630, lr: 0.001000\n",
      "Epoch [198/1000], Loss: 0.934043, test: 0.036873, lr: 0.001000\n",
      "Epoch [199/1000], Loss: 0.934584, test: 0.029350, lr: 0.001000\n",
      "Epoch [200/1000], Loss: 0.935399, test: 0.080721, lr: 0.001000\n",
      "Epoch [201/1000], Loss: 0.967462, test: 0.033532, lr: 0.001000\n",
      "Epoch [202/1000], Loss: 0.950747, test: 0.034016, lr: 0.001000\n",
      "Epoch [203/1000], Loss: 0.934665, test: 0.072838, lr: 0.001000\n",
      "Epoch [204/1000], Loss: 0.945077, test: 0.035235, lr: 0.001000\n",
      "Epoch [205/1000], Loss: 0.939375, test: 0.034258, lr: 0.001000\n",
      "Epoch [206/1000], Loss: 0.959497, test: 0.083948, lr: 0.001000\n",
      "Epoch [207/1000], Loss: 0.944330, test: 0.034974, lr: 0.001000\n",
      "Epoch [208/1000], Loss: 0.935366, test: 0.032940, lr: 0.001000\n",
      "Epoch [209/1000], Loss: 0.943175, test: 0.030971, lr: 0.001000\n",
      "Epoch [210/1000], Loss: 0.938985, test: 0.054129, lr: 0.001000\n",
      "Epoch [211/1000], Loss: 0.936205, test: 0.031418, lr: 0.001000\n",
      "Epoch [212/1000], Loss: 0.937763, test: 0.039274, lr: 0.001000\n",
      "Epoch [213/1000], Loss: 0.958306, test: 0.050871, lr: 0.001000\n",
      "Epoch [214/1000], Loss: 0.946449, test: 0.031232, lr: 0.001000\n",
      "Epoch [215/1000], Loss: 0.934492, test: 0.032691, lr: 0.001000\n",
      "Epoch [216/1000], Loss: 0.936492, test: 0.050158, lr: 0.001000\n",
      "Epoch [217/1000], Loss: 0.935299, test: 0.033410, lr: 0.001000\n",
      "Epoch [218/1000], Loss: 0.930340, test: 0.033206, lr: 0.001000\n",
      "Epoch [219/1000], Loss: 0.934529, test: 0.041094, lr: 0.001000\n",
      "Epoch [220/1000], Loss: 0.935240, test: 0.038203, lr: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mappend(avg_loss)\n\u001b[1;32m---> 30\u001b[0m test_l \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_test_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mtest_loss\u001b[38;5;241m.\u001b[39mappend(test_l)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_l\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\JP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m, in \u001b[0;36mcompute_test_loss\u001b[1;34m(test_loader, model)\u001b[0m\n\u001b[0;32m      4\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, target \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(target, outputs)\n\u001b[0;32m      8\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\JP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m, in \u001b[0;36mMLayerNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 23\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Apply ReLU activation\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n",
      "File \u001b[1;32mc:\\Users\\JP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import dill\n",
    "n_samples = 20000\n",
    "in_dims = [5,100]\n",
    "noises = [1.]\n",
    "epochs = 1000\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "adjust = True\n",
    "for j,in_dim in enumerate(in_dims):\n",
    "    for noise in noises:\n",
    "        param = 1\n",
    "        train_loader, test_loader = get_loader(in_dim, noise,n_samples)\n",
    "        model = MLayerNN(in_dim, 1, depth = 3 , N_params = 3*75000)\n",
    "        model.train()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0 \n",
    "            for batch, target in train_loader:\n",
    "                start_time = time.time()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch)\n",
    "                loss = criterion(target, outputs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            model.train_loss.append(avg_loss)\n",
    "            test_l = compute_test_loss(test_loader, model)\n",
    "            model.test_loss.append(test_l)\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.6f}, test: {test_l:.6f}, lr: {optimizer.param_groups[0]['lr']:6f}\")\n",
    "        plt.plot(model.train_loss[-50:])\n",
    "        plt.title(f'train_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        print(\"Training Complete!\")\n",
    "        #with open(f\"models/NN_2048{noise}_{in_dim}.dill\", \"wb\") as f:\n",
    "        #    dill.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
